{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SubKube Developers. Here you will find all relevant information to deploy your application on SubKube. Getting Started Get started using Subkube in less then 1 minute, using our Getting Started Guide . Concepts Learn about the concepts used in Subkube and how they relate to traditional Kubernetes concepts and resources. Project Workloads Guides Whether you're an experienced Kubernetes App Developer or just getting started, our Guides help you understand Subkube and Kubernetes, and guide you through setting up some more complex deployments on Subkube. Kubernetes for Dummies Deploy from Docker Compose using Kompose Deploy using Helm Integrating with CI/CD Automated Deployment using Github Actions Automated Deployment using Gitlab CI/CD Security Configure a Pod to run with a limited SecurityContext Using Ingress with CertManager API Documentation Use the Subkube API to hook into your existing development processes. Subctl Documentation about our command line client, subctl","title":"Home"},{"location":"#getting-started","text":"Get started using Subkube in less then 1 minute, using our Getting Started Guide .","title":"Getting Started"},{"location":"#concepts","text":"Learn about the concepts used in Subkube and how they relate to traditional Kubernetes concepts and resources. Project Workloads","title":"Concepts"},{"location":"#guides","text":"Whether you're an experienced Kubernetes App Developer or just getting started, our Guides help you understand Subkube and Kubernetes, and guide you through setting up some more complex deployments on Subkube.","title":"Guides"},{"location":"#kubernetes-for-dummies","text":"Deploy from Docker Compose using Kompose Deploy using Helm","title":"Kubernetes for Dummies"},{"location":"#integrating-with-cicd","text":"Automated Deployment using Github Actions Automated Deployment using Gitlab CI/CD","title":"Integrating with CI/CD"},{"location":"#security","text":"Configure a Pod to run with a limited SecurityContext Using Ingress with CertManager","title":"Security"},{"location":"#api-documentation","text":"Use the Subkube API to hook into your existing development processes.","title":"API Documentation"},{"location":"#subctl","text":"Documentation about our command line client, subctl","title":"Subctl"},{"location":"cli/","text":"SubKube offers a simple CLI tool to interact with your SubKube Projects and Namespaces, called subctl . Installation The subctl is written in python, using click, and as such is easiest installed using pip as shown below, but self-contained binaries are available for most platforms as well. 1 pip install git+https://github.com/subkube/subctl Usage 1 2 3 4 5 6 7 8 9 10 subctl login subctl get project subctl get namespace subctl use project subctl use namespace subctl create namespace subctl create project","title":"subctl"},{"location":"cli/#installation","text":"The subctl is written in python, using click, and as such is easiest installed using pip as shown below, but self-contained binaries are available for most platforms as well. 1 pip install git+https://github.com/subkube/subctl","title":"Installation"},{"location":"cli/#usage","text":"1 2 3 4 5 6 7 8 9 10 subctl login subctl get project subctl get namespace subctl use project subctl use namespace subctl create namespace subctl create project","title":"Usage"},{"location":"getting_started/","text":"Getting Started Tip For hardcore command line users, we've created subctl , a simple CLI tool allowing you to log in to SubKube and interact with your Projects and Namespace directly from your shell, just as you would with kubectl . More about how to install subctl here Signing up To start using SubKube, sign up for an account Before you can start creating projects, you'll need to setup a billing method !!!+ tip If you wish to use subctl , don't forget to log in 1 subctl login Create a Project Once you've signed up and are logged in to SubKube, you need to create a project to deploy Namespaces and Workloads to. Subkube UI Open the Projects page and click the Create Project button. subctl 1 subctl create project <PROJECT-NAME> Create a Namespace After creating a project, we need to create a Kubernetes Namespace to deploy our Workloads to. Subkube UI This can easily be done from a Project page, by clicking the Create Namespace button. subctl 1 subctl create namespace <NAMESPACE-NAME> -p <PROJECT-UUID> Setup Kubectl In order to use kubectl , we need to setup our kubeconfig . Subkube UI On your projects' page you will find the Kubeconfig card, which allows you to download the kubeconfig as a file, which will be called subkubeconfig , or you can show the kubeconfig for inspection or manual copy-pasting / distribution. subctl 1 subctl use project <project-uuid> Deploy workloads Once we have our project, namespace and kubectl configured, we can check to see if everything works by running a test pod interactively. As you can notice - we've set a securityContext using overrides, please see Concept: Workloads for more on SecurityContext and PodSecurityPolicies in Subkube - we've set limits for the pod using --limits , please see Concept: Projects for more on Resource Limits in Subkube 1 2 3 4 5 kubectl run -n <YOUR-NAMESPACE> -i --tty busybox --image = busybox \\ --overrides = '{\"spec\":{\"securityContext\":{\"runAsUser\":65534}}}' \\ --limits = 'cpu=100m,memory=100Mi' --rm sh $ echo 'hello world!' hello world!","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"Tip For hardcore command line users, we've created subctl , a simple CLI tool allowing you to log in to SubKube and interact with your Projects and Namespace directly from your shell, just as you would with kubectl . More about how to install subctl here","title":"Getting Started"},{"location":"getting_started/#signing-up","text":"To start using SubKube, sign up for an account Before you can start creating projects, you'll need to setup a billing method !!!+ tip If you wish to use subctl , don't forget to log in 1 subctl login","title":"Signing up"},{"location":"getting_started/#create-a-project","text":"Once you've signed up and are logged in to SubKube, you need to create a project to deploy Namespaces and Workloads to. Subkube UI Open the Projects page and click the Create Project button. subctl 1 subctl create project <PROJECT-NAME>","title":"Create a Project"},{"location":"getting_started/#create-a-namespace","text":"After creating a project, we need to create a Kubernetes Namespace to deploy our Workloads to. Subkube UI This can easily be done from a Project page, by clicking the Create Namespace button. subctl 1 subctl create namespace <NAMESPACE-NAME> -p <PROJECT-UUID>","title":"Create a Namespace"},{"location":"getting_started/#setup-kubectl","text":"In order to use kubectl , we need to setup our kubeconfig . Subkube UI On your projects' page you will find the Kubeconfig card, which allows you to download the kubeconfig as a file, which will be called subkubeconfig , or you can show the kubeconfig for inspection or manual copy-pasting / distribution. subctl 1 subctl use project <project-uuid>","title":"Setup Kubectl"},{"location":"getting_started/#deploy-workloads","text":"Once we have our project, namespace and kubectl configured, we can check to see if everything works by running a test pod interactively. As you can notice - we've set a securityContext using overrides, please see Concept: Workloads for more on SecurityContext and PodSecurityPolicies in Subkube - we've set limits for the pod using --limits , please see Concept: Projects for more on Resource Limits in Subkube 1 2 3 4 5 kubectl run -n <YOUR-NAMESPACE> -i --tty busybox --image = busybox \\ --overrides = '{\"spec\":{\"securityContext\":{\"runAsUser\":65534}}}' \\ --limits = 'cpu=100m,memory=100Mi' --rm sh $ echo 'hello world!' hello world!","title":"Deploy workloads"},{"location":"soon/","text":"Note Coming soon!","title":"Reference"},{"location":"api/authentication/","text":"How to authenticate with SubKube API Basic Authentication Tokens","title":"Authentication"},{"location":"api/authentication/#basic-authentication","text":"","title":"Basic Authentication"},{"location":"api/authentication/#tokens","text":"","title":"Tokens"},{"location":"api/reference/","text":"Resource API Path Projects https://api.subku.be/projects.json Project Namespaces https://api.subku.be/projects/.../namespaces.json","title":"Reference"},{"location":"concepts/project/","text":"Project In SubKube, your Kubernetes Namespaces are organised inside a Project. Project can be personal, or belong to an Organization. Namespaces Namespaces are used in Kubernetes to group Workloads together. Namespaces can operate in various levels of isolation. In SubKube, this is configurable per project. More about namespaces can be found in the Kubernetes docs Resource Quota Resource Quota are set on namespaces by Rancher based on the set Project limits. More about Resource Quotas in the Rancher (on which Subkube is built) docs Billing At Subkube, you are charged for reservation resources as specified by the limits of a project. These limits can be changed on the fly. Each hour, we calculate the total of resource limits for a certain resource type during that hour, and charge that to your subscription. If you change the limits for a project, or delete a project within that hour period, you're subscription will be charged for the applicable part of the hour, multiplied by the resource limit. Persistent Volumes Are available and can be created using normal PVC's. Storage is implemented using Longhorn CSI. More about Persistent Volumes and associated Claims can be found in the Kubernetes docs Ingress In Kubernetes, Ingress objects are used to allow your application to be accessed by the outside world, by using a reverse proxy. You can read more about the Ingress concept in the Kubernetes docs The Nginx Ingress Controller is available for all users of Subkube, including certmanager, to allow SSL certificates to be set up automatically using Let's Encrypt. The Nginx Ingress Controller allows for very flexible configuration, for a full list of all options please see the nginx-ingress docs","title":"Project"},{"location":"concepts/project/#project","text":"In SubKube, your Kubernetes Namespaces are organised inside a Project. Project can be personal, or belong to an Organization.","title":"Project"},{"location":"concepts/project/#namespaces","text":"Namespaces are used in Kubernetes to group Workloads together. Namespaces can operate in various levels of isolation. In SubKube, this is configurable per project. More about namespaces can be found in the Kubernetes docs","title":"Namespaces"},{"location":"concepts/project/#resource-quota","text":"Resource Quota are set on namespaces by Rancher based on the set Project limits. More about Resource Quotas in the Rancher (on which Subkube is built) docs","title":"Resource Quota"},{"location":"concepts/project/#billing","text":"At Subkube, you are charged for reservation resources as specified by the limits of a project. These limits can be changed on the fly. Each hour, we calculate the total of resource limits for a certain resource type during that hour, and charge that to your subscription. If you change the limits for a project, or delete a project within that hour period, you're subscription will be charged for the applicable part of the hour, multiplied by the resource limit.","title":"Billing"},{"location":"concepts/project/#persistent-volumes","text":"Are available and can be created using normal PVC's. Storage is implemented using Longhorn CSI. More about Persistent Volumes and associated Claims can be found in the Kubernetes docs","title":"Persistent Volumes"},{"location":"concepts/project/#ingress","text":"In Kubernetes, Ingress objects are used to allow your application to be accessed by the outside world, by using a reverse proxy. You can read more about the Ingress concept in the Kubernetes docs The Nginx Ingress Controller is available for all users of Subkube, including certmanager, to allow SSL certificates to be set up automatically using Let's Encrypt. The Nginx Ingress Controller allows for very flexible configuration, for a full list of all options please see the nginx-ingress docs","title":"Ingress"},{"location":"concepts/security/","text":"As a shared tenant platform, Subkube takes security very seriously. Luckily, the container & kubernetes ecosystem have been designed from the ground up with security on various levels in mind. Access Control Security starts at the front door. The Subkube platform uses state-of-the-art technology to ensure a user's identity before allowing access to any of our systems and applications. 2 Factor Authentication In general it's good habit to enable 2 factor authentication for your accounts whenever possible. Subkube offers a basic TOTP implementation, allowing you to use most Authenticator apps to generate tokens allowing you to login using the application interface or our subctl CLI. During 2FA setup, a backup token will be generated and shown, allowing you to regain access to your account if for some reason your TOTP applications is no longer able to produce valid tokens for your Subkube account (a lost phone for instance). Be sure to store the backup token in a safe place, to ensure access to your Subkube account at all times. API Keys In modern day IT, we want to automate everything, but not at the expense of security. Fine-grained access control for automated processes is key in achieving this. Subkube implements a flexible way to create API tokens, to be consumed by various application, such as 3rd party services like Github and Gitlab, scripts running locally or on premise, but also in-cluster, to be used by auto-scaling tools for example. Tokens can be revoked at any time easily from the Subkube interface. Workload Isolation As we briefly outlined in Workloads , Subkube kubernetes clusters enforce certain policies and restrictions on the workloads customers run, the most important of which are the following: Restricted Pod Security Policy All namespaces created on Subkube have a 'restricted' PodSecurityPolicy applied by default. Amongst other things, this prohibits containers from running as the root user, and all security implications this has. For most applications, and well designed container images, this shouldn't pose a problem. As containers can not run as root, most processes will be unable to, for instance, allocate a port lower than 1000 . Also, package management isn't available while not running as user root . Though this might seem problematic at first, it really shouldn't be. If a container has been though-out and built properly, applications will never need these kinds of permissions in order to run, and as such, neither does your container. For instance, if you feel you need systemd or init inside your container, you're probably trying to put various processes in a single container, which is bad practise, as Kubernetes offers you the Pod concept, specifically to deal with these kinds of \"groups\" of processes. Namespace Network Isolation As in any decent microservices architecture services need to be able to talk to oneanother, providing network connectivity between pods is vital to any Kubernetes deployment. However, as Subkube uses a shared tenancy principle, we do need to seperate traffic from various customers. We accomplish this by isolating traffic to a namespace, using Network Policies. Our clusters run Canal, a mixed implementation of Flannel to provide inter-node and inter-pod connectivity, combined with Calico to provide network policy management & enforcement. By default, when a Namespace is created inside a project, NetworkPolicies are defined inside the namespace to allow traffic between the namespaces inside your project, and namespaces hosting Subkube shared services, such as the Ingress Controller. Cluster Operations Hardening All our Kubernetes nodes run on selinux-enforced CentOS 8 Core, hardened according to enterprise standard. Our Kubernetes deployment takes all Kubernetes hardening considerations into account, and we run daily automated audits to ensure our clusters are compliant with the CIS Hardened benchmark. End-to-end Encryption We use Wireguard, both to connect our control-plane and worker nodes together, and to connect our frontend and APIs to our backing Kubernetes clusters, hosted across the internet. Wireguard is state-of-the-art and allows for full encryption with minimal overhead, to ensure fast response times across the platform. Let's Encrypt Integration All projects utilising Ingress for connectivity benefit from out Let's Encrypt integration, which offers a wildcard certificate to be used by ingress resources in project namespaces, so we can properly enforce SSL for connecting to Ingress-exposed services.","title":"Security"},{"location":"concepts/security/#access-control","text":"Security starts at the front door. The Subkube platform uses state-of-the-art technology to ensure a user's identity before allowing access to any of our systems and applications.","title":"Access Control"},{"location":"concepts/security/#2-factor-authentication","text":"In general it's good habit to enable 2 factor authentication for your accounts whenever possible. Subkube offers a basic TOTP implementation, allowing you to use most Authenticator apps to generate tokens allowing you to login using the application interface or our subctl CLI. During 2FA setup, a backup token will be generated and shown, allowing you to regain access to your account if for some reason your TOTP applications is no longer able to produce valid tokens for your Subkube account (a lost phone for instance). Be sure to store the backup token in a safe place, to ensure access to your Subkube account at all times.","title":"2 Factor Authentication"},{"location":"concepts/security/#api-keys","text":"In modern day IT, we want to automate everything, but not at the expense of security. Fine-grained access control for automated processes is key in achieving this. Subkube implements a flexible way to create API tokens, to be consumed by various application, such as 3rd party services like Github and Gitlab, scripts running locally or on premise, but also in-cluster, to be used by auto-scaling tools for example. Tokens can be revoked at any time easily from the Subkube interface.","title":"API Keys"},{"location":"concepts/security/#workload-isolation","text":"As we briefly outlined in Workloads , Subkube kubernetes clusters enforce certain policies and restrictions on the workloads customers run, the most important of which are the following:","title":"Workload Isolation"},{"location":"concepts/security/#restricted-pod-security-policy","text":"All namespaces created on Subkube have a 'restricted' PodSecurityPolicy applied by default. Amongst other things, this prohibits containers from running as the root user, and all security implications this has. For most applications, and well designed container images, this shouldn't pose a problem. As containers can not run as root, most processes will be unable to, for instance, allocate a port lower than 1000 . Also, package management isn't available while not running as user root . Though this might seem problematic at first, it really shouldn't be. If a container has been though-out and built properly, applications will never need these kinds of permissions in order to run, and as such, neither does your container. For instance, if you feel you need systemd or init inside your container, you're probably trying to put various processes in a single container, which is bad practise, as Kubernetes offers you the Pod concept, specifically to deal with these kinds of \"groups\" of processes.","title":"Restricted Pod Security Policy"},{"location":"concepts/security/#namespace-network-isolation","text":"As in any decent microservices architecture services need to be able to talk to oneanother, providing network connectivity between pods is vital to any Kubernetes deployment. However, as Subkube uses a shared tenancy principle, we do need to seperate traffic from various customers. We accomplish this by isolating traffic to a namespace, using Network Policies. Our clusters run Canal, a mixed implementation of Flannel to provide inter-node and inter-pod connectivity, combined with Calico to provide network policy management & enforcement. By default, when a Namespace is created inside a project, NetworkPolicies are defined inside the namespace to allow traffic between the namespaces inside your project, and namespaces hosting Subkube shared services, such as the Ingress Controller.","title":"Namespace Network Isolation"},{"location":"concepts/security/#cluster-operations","text":"","title":"Cluster Operations"},{"location":"concepts/security/#hardening","text":"All our Kubernetes nodes run on selinux-enforced CentOS 8 Core, hardened according to enterprise standard. Our Kubernetes deployment takes all Kubernetes hardening considerations into account, and we run daily automated audits to ensure our clusters are compliant with the CIS Hardened benchmark.","title":"Hardening"},{"location":"concepts/security/#end-to-end-encryption","text":"We use Wireguard, both to connect our control-plane and worker nodes together, and to connect our frontend and APIs to our backing Kubernetes clusters, hosted across the internet. Wireguard is state-of-the-art and allows for full encryption with minimal overhead, to ensure fast response times across the platform.","title":"End-to-end Encryption"},{"location":"concepts/security/#lets-encrypt-integration","text":"All projects utilising Ingress for connectivity benefit from out Let's Encrypt integration, which offers a wildcard certificate to be used by ingress resources in project namespaces, so we can properly enforce SSL for connecting to Ingress-exposed services.","title":"Let's Encrypt Integration"},{"location":"concepts/workloads/","text":"Workloads In Kubernetes, you run your application as a workload . There are various ways to orchestrate workloads, for instance using the deployment resource, the statefulSet or the daemonSet resource. Many apps consist of various different workloads, which need to work together in order to serve the application as a whole. Kubernetes has an applicable workload type for any of these. For more information on workloads, please checkout the dedicated Kubernetes docs Pod Security Context By default, many containers used to run as root user. This isn't an option on a shared cluster, as this could potentially allow users to \"break out\" of a running container, and gain access to the entire node. In Kubernetes this is enforced using PodSecurityPolicies. The default PodSecurityPolicy enforced on Subkube is called 'restricted' and is very comparable to those seen in large scale enterprise Kubernetes deployments. Not Available in Subkube Resource Reason Daemonset DaemonSets can not be used on SubKube as it is a shared platform, where no application should need a DaemonSet. NodePort Service Too unpredictable for production usage , incompatible with Subkube cluster firewall Container Hostport Too unpredictable for production usage , incompatible with Subkube cluster firewall Custom Resource Incompatible with shared cluster usage, as CRDs are Definition cluster-wide resources.","title":"Workloads"},{"location":"concepts/workloads/#workloads","text":"In Kubernetes, you run your application as a workload . There are various ways to orchestrate workloads, for instance using the deployment resource, the statefulSet or the daemonSet resource. Many apps consist of various different workloads, which need to work together in order to serve the application as a whole. Kubernetes has an applicable workload type for any of these. For more information on workloads, please checkout the dedicated Kubernetes docs","title":"Workloads"},{"location":"concepts/workloads/#pod-security-context","text":"By default, many containers used to run as root user. This isn't an option on a shared cluster, as this could potentially allow users to \"break out\" of a running container, and gain access to the entire node. In Kubernetes this is enforced using PodSecurityPolicies. The default PodSecurityPolicy enforced on Subkube is called 'restricted' and is very comparable to those seen in large scale enterprise Kubernetes deployments.","title":"Pod Security Context"},{"location":"concepts/workloads/#not-available-in-subkube","text":"Resource Reason Daemonset DaemonSets can not be used on SubKube as it is a shared platform, where no application should need a DaemonSet. NodePort Service Too unpredictable for production usage , incompatible with Subkube cluster firewall Container Hostport Too unpredictable for production usage , incompatible with Subkube cluster firewall Custom Resource Incompatible with shared cluster usage, as CRDs are Definition cluster-wide resources.","title":"Not Available in Subkube"},{"location":"guides/deploy-github-actions/","text":"You can use Github actions to deploy to Subkube directly from Github. Prerequisites A Namespace in a Subkube Project A Github account Steps Prepare Application For this guide we will be deploying Nextcloud, which we covered in our Kompose guide. After fetching the resources from our examples repository, you should initialize a new github repository containing our k8s manifests. Setup Kubeconfig in Github In order to deploy to Subkube using kubectl from Github Actions, we need to store a copy of our kubeconfig inside a Github environment variable. Add Kubectl action After we've setup our kubeconfig in Github, we can define an Action using this config to deploy to subkube. Define the following action inside the .github/workflows/deploy.yaml file. Commit and Deploy Now everything is setup, we can push our changes and watch them be applied to your Subkube project.","title":"Deploy github actions"},{"location":"guides/deploy-github-actions/#prerequisites","text":"A Namespace in a Subkube Project A Github account","title":"Prerequisites"},{"location":"guides/deploy-github-actions/#steps","text":"","title":"Steps"},{"location":"guides/deploy-github-actions/#prepare-application","text":"For this guide we will be deploying Nextcloud, which we covered in our Kompose guide. After fetching the resources from our examples repository, you should initialize a new github repository containing our k8s manifests.","title":"Prepare Application"},{"location":"guides/deploy-github-actions/#setup-kubeconfig-in-github","text":"In order to deploy to Subkube using kubectl from Github Actions, we need to store a copy of our kubeconfig inside a Github environment variable.","title":"Setup Kubeconfig in Github"},{"location":"guides/deploy-github-actions/#add-kubectl-action","text":"After we've setup our kubeconfig in Github, we can define an Action using this config to deploy to subkube. Define the following action inside the .github/workflows/deploy.yaml file.","title":"Add Kubectl action"},{"location":"guides/deploy-github-actions/#commit-and-deploy","text":"Now everything is setup, we can push our changes and watch them be applied to your Subkube project.","title":"Commit and Deploy"},{"location":"guides/helm/","text":"Coming soon","title":"Helm"},{"location":"guides/ingress-certmanager/","text":"In this guide, we will be creating an Ingress resource to expose a workload to the world, after which we will setup Certmanager to automatically generate and configure a TLS certificate for our Ingress . Tip Due to issues with Let's Encrypt rate limiting, we've disabled Cert Manager for now. Prerequisites Kubectl A custom domain name for which you can configure DNS Steps Deploy a workload Before we can dive in to Ingresses, we need a workload to expose through an Ingress. We'll be using a very basic echo-server Deployment and Service : deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : apps/v1 kind : Deployment metadata : creationTimestamp : null labels : app : echoserver name : echoserver spec : replicas : 1 selector : matchLabels : app : echoserver strategy : {} template : metadata : creationTimestamp : null labels : app : echoserver spec : containers : - image : ealen/echo-server:latest name : echoserver resources : limits : cpu : '100m' memory : '100Mi' env : - name : PORT value : \"8080\" securityContext : runAsUser : 65534 status : {} service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : v1 kind : Service metadata : creationTimestamp : null labels : app : echoserver name : echoserver spec : ports : - name : 8080-8080 port : 8080 protocol : TCP targetPort : 8080 selector : app : echoserver type : ClusterIP status : loadBalancer : {} Let's deploy these resources: 1 2 3 > kubectl apply -n demo -f deployment.yaml -f service.yaml deployment.apps/echoserver created service/echoserver created Setup DNS Point a domain name towards the cluster ingress by creating a cname to ... Defining an Ingress An Ingress is basically a mapping between a Domain Name and a Service , provided on Subkube using an Nginx reverse proxy, but other Ingress implementations are also available. ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : echoserver spec : rules : - host : echoserver.subku.be http : paths : - backend : serviceName : echoserver servicePort : 8080 path : / Using kubectl we can apply this Ingress , http-only for now: 1 2 > kubectl apply -n demo -f ingress.yaml ingress.extensions/echoserver created We can then use curl to check the result: 1 > curl Create an Issuer CertManager needs an Issuer to use while requesting certificates. It is recommended to use a separate staging and production issuer, while using Let's Encrypt, to ensure you don't hit any rate limits during experimentation. issuer.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : cert-manager.io/v1alpha2 kind : Issuer metadata : name : letsencrypt-staging spec : acme : # You must replace this email address with your own. # Let's Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email : testuser@subku.be server : https://acme-staging-v02.api.letsencrypt.org/directory privateKeySecretRef : # Secret resource that will be used to store the account's private key. name : demo-issuer-account-key # Add a single challenge solver, HTTP01 using nginx solvers : - http01 : ingress : class : nginx 1 2 > kubectl apply -n demo -f issuer.yaml issuer.cert-manager.io/letsencrypt-staging created Update Ingress to use Issuer Now we can update our previously created Ingress resource to make use of the new Issuer to automatically request, verify, and setup TLS certificates. ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : # add an annotation indicating the issuer to use. cert-manager.io/issuer : letsencrypt-staging name : echoserver spec : rules : - host : echoserver.subku.be http : paths : - backend : serviceName : echoserver servicePort : 8080 path : / tls : # < placing a host in the TLS config will indicate a certificate should be created - hosts : - echoserver.subku.be secretName : echoserver-cert 1 2 > kubectl apply -n demo -f ingress.yaml ingress.extensions/echoserver configured After we've applied the updated Ingress , we need to give CertManager some time to setup the CertificateRequest and update the appropriate Certificate resource: 1 > kubectl get certificate -n demo -w Verify Certificate Further Reading","title":"Using Ingress with CertManager"},{"location":"guides/ingress-certmanager/#prerequisites","text":"Kubectl A custom domain name for which you can configure DNS","title":"Prerequisites"},{"location":"guides/ingress-certmanager/#steps","text":"","title":"Steps"},{"location":"guides/ingress-certmanager/#deploy-a-workload","text":"Before we can dive in to Ingresses, we need a workload to expose through an Ingress. We'll be using a very basic echo-server Deployment and Service : deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : apps/v1 kind : Deployment metadata : creationTimestamp : null labels : app : echoserver name : echoserver spec : replicas : 1 selector : matchLabels : app : echoserver strategy : {} template : metadata : creationTimestamp : null labels : app : echoserver spec : containers : - image : ealen/echo-server:latest name : echoserver resources : limits : cpu : '100m' memory : '100Mi' env : - name : PORT value : \"8080\" securityContext : runAsUser : 65534 status : {} service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion : v1 kind : Service metadata : creationTimestamp : null labels : app : echoserver name : echoserver spec : ports : - name : 8080-8080 port : 8080 protocol : TCP targetPort : 8080 selector : app : echoserver type : ClusterIP status : loadBalancer : {} Let's deploy these resources: 1 2 3 > kubectl apply -n demo -f deployment.yaml -f service.yaml deployment.apps/echoserver created service/echoserver created","title":"Deploy a workload"},{"location":"guides/ingress-certmanager/#setup-dns","text":"Point a domain name towards the cluster ingress by creating a cname to ...","title":"Setup DNS"},{"location":"guides/ingress-certmanager/#defining-an-ingress","text":"An Ingress is basically a mapping between a Domain Name and a Service , provided on Subkube using an Nginx reverse proxy, but other Ingress implementations are also available. ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : echoserver spec : rules : - host : echoserver.subku.be http : paths : - backend : serviceName : echoserver servicePort : 8080 path : / Using kubectl we can apply this Ingress , http-only for now: 1 2 > kubectl apply -n demo -f ingress.yaml ingress.extensions/echoserver created We can then use curl to check the result: 1 > curl","title":"Defining an Ingress"},{"location":"guides/ingress-certmanager/#create-an-issuer","text":"CertManager needs an Issuer to use while requesting certificates. It is recommended to use a separate staging and production issuer, while using Let's Encrypt, to ensure you don't hit any rate limits during experimentation. issuer.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : cert-manager.io/v1alpha2 kind : Issuer metadata : name : letsencrypt-staging spec : acme : # You must replace this email address with your own. # Let's Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email : testuser@subku.be server : https://acme-staging-v02.api.letsencrypt.org/directory privateKeySecretRef : # Secret resource that will be used to store the account's private key. name : demo-issuer-account-key # Add a single challenge solver, HTTP01 using nginx solvers : - http01 : ingress : class : nginx 1 2 > kubectl apply -n demo -f issuer.yaml issuer.cert-manager.io/letsencrypt-staging created","title":"Create an Issuer"},{"location":"guides/ingress-certmanager/#update-ingress-to-use-issuer","text":"Now we can update our previously created Ingress resource to make use of the new Issuer to automatically request, verify, and setup TLS certificates. ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : # add an annotation indicating the issuer to use. cert-manager.io/issuer : letsencrypt-staging name : echoserver spec : rules : - host : echoserver.subku.be http : paths : - backend : serviceName : echoserver servicePort : 8080 path : / tls : # < placing a host in the TLS config will indicate a certificate should be created - hosts : - echoserver.subku.be secretName : echoserver-cert 1 2 > kubectl apply -n demo -f ingress.yaml ingress.extensions/echoserver configured After we've applied the updated Ingress , we need to give CertManager some time to setup the CertificateRequest and update the appropriate Certificate resource: 1 > kubectl get certificate -n demo -w","title":"Update Ingress to use Issuer"},{"location":"guides/ingress-certmanager/#verify-certificate","text":"","title":"Verify Certificate"},{"location":"guides/ingress-certmanager/#further-reading","text":"","title":"Further Reading"},{"location":"guides/kompose/","text":"Kompose is a simple utility you can use to convert your existing docker-compose files into Kubernetes Workloads. In this guide, we'll walk you through using the kompose to convert a NextCloud Compose file to a full Kubernetes workload. Prerequisites A Namespace in a Subkube Project Kubectl Optional: Kompose (we'll install kompose as part of this guide) Steps Installing Kompose On Linux or macOS, installing compose is easy as: 1 2 3 4 5 6 7 8 # Linux curl -L https://github.com/kubernetes/kompose/releases/download/v1.22.0/kompose-linux-amd64 -o kompose # macOS curl -L https://github.com/kubernetes/kompose/releases/download/v1.22.0/kompose-darwin-amd64 -o kompose chmod +x kompose sudo mv ./kompose /usr/local/bin/kompose More installation instructions, including for windows, can be found on the kompose.io/installation page Inspecting our Compose file For this guide, we will be using the following docker-compose.yaml file to deploy a NextCloud instance, including a MariaDB and Nginx instance: docker-compose.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 version : '2' services : app : image : nextcloud:fpm-alpine links : - db ports : - 9000 volumes : - nextcloud:/var/www/html environment : - MYSQL_HOST=db - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_ROOT_PASSWORD=nextcloud - MYSQL_PASSWORD=nextcloud restart : always web : image : nginxinc/nginx-unprivileged ports : - 8080:8080 links : - app volumes : - ./nginx.conf:/etc/nginx/nginx.conf:ro volumes_from : - app restart : always db : image : mariadb command : --transaction-isolation=READ-COMMITTED --binlog-format=ROW restart : always ports : - 3306 volumes : - db:/var/lib/mysql environment : - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_ROOT_PASSWORD=nextcloud - MYSQL_PASSWORD=nextcloud volumes : nextcloud : db : As you can see it is using the nextcloud:fpm image to run the NextCloud php application, and an nginxinc/nginx-unprivileged image to run Nginx as non-root on port 8080 . Running the Conversion The kompose utilty has various capabilities, including the option to generate a Helm Chart from a Compose file, create Openshift-compliant resources, or create ReplicationController or DaemonSet workloads. In this example, we will use the --out option to tell kompose to put the generated manifests in the output directory, which we have to create manually before running kompose : 1 2 3 4 5 6 7 8 9 10 11 12 13 > mkdir output > kompose convert --out output/ WARN Unsupported root level volumes key - ignoring WARN Volume mount on the host \"./nginx.conf\" isn \\' t supported - ignoring path on the host INFO Kubernetes file \"output/app-service.yaml\" created INFO Kubernetes file \"output/db-service.yaml\" created INFO Kubernetes file \"output/web-service.yaml\" created INFO Kubernetes file \"output/app-deployment.yaml\" created INFO Kubernetes file \"output/nextcloud-persistentvolumeclaim.yaml\" created INFO Kubernetes file \"output/db-deployment.yaml\" created INFO Kubernetes file \"output/db-persistentvolumeclaim.yaml\" created INFO Kubernetes file \"output/web-deployment.yaml\" created INFO Kubernetes file \"output/web-claim0-persistentvolumeclaim.yaml\" created As you can see, Kompose has created both a Deployment and a Service for each entry in our Compose file. You also see a warning about the nginx.conf file used for the Nginx web container, which we will address in the upcoming section. Updating the generated resources Before we can apply these resources, we want to make some changes. Add our Configuration We use a custom Nginx Configuration for the nginx container serving as HTTP proxy to Nextloud's php-fpm app container. As you cannot simply mount a file inside a container on Kubernetes, we need to convert the nginx.conf file to a ConfigMap . Luckily, kubectl can do this largely for us: 1 2 3 4 kubectl create configmap -o yaml --dry-run \\ web \\ --from-file nginx.conf \\ > output/web-configmap.yaml Now we should replace the generated web-claim0 PersistentVolumeClaim with our ConfigMap : web-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : web name : web spec : replicas : 1 selector : matchLabels : io.kompose.service : web strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : web spec : containers : - image : nginxinc/nginx-unprivileged name : web envFrom : - configMapRef : name : web ports : - containerPort : 8080 resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - name : config-volume mountPath : /etc/nginx/nginx.conf subPath : nginx.conf - mountPath : /var/www/html name : nextcloud restartPolicy : Always volumes : - name : config-volume configMap : name : web - name : nextcloud persistentVolumeClaim : claimName : nextcloud status : {} Do not forget to remove the unneeded web-claim0 PersistentVolumeClaim manifest: 1 rm output/web-claim0-persistentvolumeclaim.yaml Use a Secret In our docker-compose.yaml , we specified the MySQL connection parameters directly as environment variables to the container. In Kubernetes, we use Secrets for this sort of thing. Let's setup a secret we can use for our MariaDB deployment, using kubectl : 1 2 3 4 5 6 kubectl create secret generic -oyaml --dry-run \\ db \\ --from-literal = MYSQL_USER = nextcloud \\ --from-literal = MYSQL_ROOT_PASSWORD = nextcloud \\ --from-literal = MYSQL_PASSWORD = nextcloud \\ > output/db-secret.yaml Inside our Deployments , we can use the envFrom directive to tell Kubernetes which Secret to use for the app and db workloads: app-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : app name : app spec : replicas : 1 selector : matchLabels : io.kompose.service : app strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : app spec : containers : - env : - name : MYSQL_DATABASE value : nextcloud - name : MYSQL_HOST value : db envFrom : - secretRef : name : db image : nextcloud:fpm name : app resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - mountPath : /var/www/html name : nextcloud restartPolicy : Always volumes : - name : nextcloud persistentVolumeClaim : claimName : nextcloud status : {} db-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : db name : db spec : replicas : 1 selector : matchLabels : io.kompose.service : db strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : db spec : containers : - args : - --transaction-isolation=READ-COMMITTED - --binlog-format=ROW env : - name : MYSQL_DATABASE value : nextcloud envFrom : - secretRef : name : db image : mariadb name : db resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - mountPath : /var/lib/mysql name : db restartPolicy : Always volumes : - name : db persistentVolumeClaim : claimName : db status : {} Setting Resources & Security Contexts As with any workload running on Subkube, we should set appropriate resources for the deployments, as well as securityContexts. app-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : app name : app spec : replicas : 1 selector : matchLabels : io.kompose.service : app strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : app spec : containers : - env : - name : MYSQL_DATABASE value : nextcloud - name : MYSQL_HOST value : db envFrom : - secretRef : name : db image : nextcloud:fpm name : app resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - mountPath : /var/www/html name : nextcloud restartPolicy : Always volumes : - name : nextcloud persistentVolumeClaim : claimName : nextcloud status : {} db-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : db name : db spec : replicas : 1 selector : matchLabels : io.kompose.service : db strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : db spec : containers : - args : - --transaction-isolation=READ-COMMITTED - --binlog-format=ROW env : - name : MYSQL_DATABASE value : nextcloud envFrom : - secretRef : name : db image : mariadb name : db resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - mountPath : /var/lib/mysql name : db restartPolicy : Always volumes : - name : db persistentVolumeClaim : claimName : db status : {} web-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : web name : web spec : replicas : 1 selector : matchLabels : io.kompose.service : web strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : web spec : containers : - image : nginxinc/nginx-unprivileged name : web envFrom : - configMapRef : name : web ports : - containerPort : 8080 resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - name : config-volume mountPath : /etc/nginx/nginx.conf subPath : nginx.conf - mountPath : /var/www/html name : nextcloud restartPolicy : Always volumes : - name : config-volume configMap : name : web - name : nextcloud persistentVolumeClaim : claimName : nextcloud status : {} Apply and Verify After we have made these changes, we can deploy the resources: 1 2 3 4 5 6 7 8 9 10 11 > kubectl apply -n demo -f output/ deployment.apps/app created service/app created deployment.apps/db created persistentvolumeclaim/db created secret/db created service/db created persistentvolumeclaim/nextcloud created configmap/web created deployment.apps/web created service/web created We can monitor our Deployments' to see them progressing: 1 kubectl get deployment -n demo -w Once everything is ready, we can setup a port-forward to access NextCloud and trigger the setup process, by creating an Admin user. 1 kubectl port-forward service/web -n demo 8080 :8080 More Resources & Further Reading Kompose Docs: User Guide Kompose Docs: Architecture","title":"Deploy from Docker Compose using Kompose"},{"location":"guides/kompose/#prerequisites","text":"A Namespace in a Subkube Project Kubectl Optional: Kompose (we'll install kompose as part of this guide)","title":"Prerequisites"},{"location":"guides/kompose/#steps","text":"","title":"Steps"},{"location":"guides/kompose/#installing-kompose","text":"On Linux or macOS, installing compose is easy as: 1 2 3 4 5 6 7 8 # Linux curl -L https://github.com/kubernetes/kompose/releases/download/v1.22.0/kompose-linux-amd64 -o kompose # macOS curl -L https://github.com/kubernetes/kompose/releases/download/v1.22.0/kompose-darwin-amd64 -o kompose chmod +x kompose sudo mv ./kompose /usr/local/bin/kompose More installation instructions, including for windows, can be found on the kompose.io/installation page","title":"Installing Kompose"},{"location":"guides/kompose/#inspecting-our-compose-file","text":"For this guide, we will be using the following docker-compose.yaml file to deploy a NextCloud instance, including a MariaDB and Nginx instance: docker-compose.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 version : '2' services : app : image : nextcloud:fpm-alpine links : - db ports : - 9000 volumes : - nextcloud:/var/www/html environment : - MYSQL_HOST=db - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_ROOT_PASSWORD=nextcloud - MYSQL_PASSWORD=nextcloud restart : always web : image : nginxinc/nginx-unprivileged ports : - 8080:8080 links : - app volumes : - ./nginx.conf:/etc/nginx/nginx.conf:ro volumes_from : - app restart : always db : image : mariadb command : --transaction-isolation=READ-COMMITTED --binlog-format=ROW restart : always ports : - 3306 volumes : - db:/var/lib/mysql environment : - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_ROOT_PASSWORD=nextcloud - MYSQL_PASSWORD=nextcloud volumes : nextcloud : db : As you can see it is using the nextcloud:fpm image to run the NextCloud php application, and an nginxinc/nginx-unprivileged image to run Nginx as non-root on port 8080 .","title":"Inspecting our Compose file"},{"location":"guides/kompose/#running-the-conversion","text":"The kompose utilty has various capabilities, including the option to generate a Helm Chart from a Compose file, create Openshift-compliant resources, or create ReplicationController or DaemonSet workloads. In this example, we will use the --out option to tell kompose to put the generated manifests in the output directory, which we have to create manually before running kompose : 1 2 3 4 5 6 7 8 9 10 11 12 13 > mkdir output > kompose convert --out output/ WARN Unsupported root level volumes key - ignoring WARN Volume mount on the host \"./nginx.conf\" isn \\' t supported - ignoring path on the host INFO Kubernetes file \"output/app-service.yaml\" created INFO Kubernetes file \"output/db-service.yaml\" created INFO Kubernetes file \"output/web-service.yaml\" created INFO Kubernetes file \"output/app-deployment.yaml\" created INFO Kubernetes file \"output/nextcloud-persistentvolumeclaim.yaml\" created INFO Kubernetes file \"output/db-deployment.yaml\" created INFO Kubernetes file \"output/db-persistentvolumeclaim.yaml\" created INFO Kubernetes file \"output/web-deployment.yaml\" created INFO Kubernetes file \"output/web-claim0-persistentvolumeclaim.yaml\" created As you can see, Kompose has created both a Deployment and a Service for each entry in our Compose file. You also see a warning about the nginx.conf file used for the Nginx web container, which we will address in the upcoming section.","title":"Running the Conversion"},{"location":"guides/kompose/#updating-the-generated-resources","text":"Before we can apply these resources, we want to make some changes.","title":"Updating the generated resources"},{"location":"guides/kompose/#add-our-configuration","text":"We use a custom Nginx Configuration for the nginx container serving as HTTP proxy to Nextloud's php-fpm app container. As you cannot simply mount a file inside a container on Kubernetes, we need to convert the nginx.conf file to a ConfigMap . Luckily, kubectl can do this largely for us: 1 2 3 4 kubectl create configmap -o yaml --dry-run \\ web \\ --from-file nginx.conf \\ > output/web-configmap.yaml Now we should replace the generated web-claim0 PersistentVolumeClaim with our ConfigMap : web-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : web name : web spec : replicas : 1 selector : matchLabels : io.kompose.service : web strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : web spec : containers : - image : nginxinc/nginx-unprivileged name : web envFrom : - configMapRef : name : web ports : - containerPort : 8080 resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - name : config-volume mountPath : /etc/nginx/nginx.conf subPath : nginx.conf - mountPath : /var/www/html name : nextcloud restartPolicy : Always volumes : - name : config-volume configMap : name : web - name : nextcloud persistentVolumeClaim : claimName : nextcloud status : {} Do not forget to remove the unneeded web-claim0 PersistentVolumeClaim manifest: 1 rm output/web-claim0-persistentvolumeclaim.yaml","title":"Add our Configuration"},{"location":"guides/kompose/#use-a-secret","text":"In our docker-compose.yaml , we specified the MySQL connection parameters directly as environment variables to the container. In Kubernetes, we use Secrets for this sort of thing. Let's setup a secret we can use for our MariaDB deployment, using kubectl : 1 2 3 4 5 6 kubectl create secret generic -oyaml --dry-run \\ db \\ --from-literal = MYSQL_USER = nextcloud \\ --from-literal = MYSQL_ROOT_PASSWORD = nextcloud \\ --from-literal = MYSQL_PASSWORD = nextcloud \\ > output/db-secret.yaml Inside our Deployments , we can use the envFrom directive to tell Kubernetes which Secret to use for the app and db workloads: app-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : app name : app spec : replicas : 1 selector : matchLabels : io.kompose.service : app strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : app spec : containers : - env : - name : MYSQL_DATABASE value : nextcloud - name : MYSQL_HOST value : db envFrom : - secretRef : name : db image : nextcloud:fpm name : app resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - mountPath : /var/www/html name : nextcloud restartPolicy : Always volumes : - name : nextcloud persistentVolumeClaim : claimName : nextcloud status : {} db-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : db name : db spec : replicas : 1 selector : matchLabels : io.kompose.service : db strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : db spec : containers : - args : - --transaction-isolation=READ-COMMITTED - --binlog-format=ROW env : - name : MYSQL_DATABASE value : nextcloud envFrom : - secretRef : name : db image : mariadb name : db resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - mountPath : /var/lib/mysql name : db restartPolicy : Always volumes : - name : db persistentVolumeClaim : claimName : db status : {}","title":"Use a Secret"},{"location":"guides/kompose/#setting-resources-security-contexts","text":"As with any workload running on Subkube, we should set appropriate resources for the deployments, as well as securityContexts. app-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : app name : app spec : replicas : 1 selector : matchLabels : io.kompose.service : app strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : app spec : containers : - env : - name : MYSQL_DATABASE value : nextcloud - name : MYSQL_HOST value : db envFrom : - secretRef : name : db image : nextcloud:fpm name : app resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - mountPath : /var/www/html name : nextcloud restartPolicy : Always volumes : - name : nextcloud persistentVolumeClaim : claimName : nextcloud status : {} db-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : db name : db spec : replicas : 1 selector : matchLabels : io.kompose.service : db strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : db spec : containers : - args : - --transaction-isolation=READ-COMMITTED - --binlog-format=ROW env : - name : MYSQL_DATABASE value : nextcloud envFrom : - secretRef : name : db image : mariadb name : db resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - mountPath : /var/lib/mysql name : db restartPolicy : Always volumes : - name : db persistentVolumeClaim : claimName : db status : {} web-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion : apps/v1 kind : Deployment metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : web name : web spec : replicas : 1 selector : matchLabels : io.kompose.service : web strategy : type : Recreate template : metadata : annotations : kompose.cmd : kompose convert --out output/ kompose.version : 1.22.0 (HEAD) creationTimestamp : null labels : io.kompose.service : web spec : containers : - image : nginxinc/nginx-unprivileged name : web envFrom : - configMapRef : name : web ports : - containerPort : 8080 resources : limits : cpu : '200m' memory : '200Mi' requests : cpu : '100m' memory : '100Mi' volumeMounts : - name : config-volume mountPath : /etc/nginx/nginx.conf subPath : nginx.conf - mountPath : /var/www/html name : nextcloud restartPolicy : Always volumes : - name : config-volume configMap : name : web - name : nextcloud persistentVolumeClaim : claimName : nextcloud status : {}","title":"Setting Resources &amp; Security Contexts"},{"location":"guides/kompose/#apply-and-verify","text":"After we have made these changes, we can deploy the resources: 1 2 3 4 5 6 7 8 9 10 11 > kubectl apply -n demo -f output/ deployment.apps/app created service/app created deployment.apps/db created persistentvolumeclaim/db created secret/db created service/db created persistentvolumeclaim/nextcloud created configmap/web created deployment.apps/web created service/web created We can monitor our Deployments' to see them progressing: 1 kubectl get deployment -n demo -w Once everything is ready, we can setup a port-forward to access NextCloud and trigger the setup process, by creating an Admin user. 1 kubectl port-forward service/web -n demo 8080 :8080","title":"Apply and Verify"},{"location":"guides/kompose/#more-resources-further-reading","text":"Kompose Docs: User Guide Kompose Docs: Architecture","title":"More Resources &amp; Further Reading"},{"location":"guides/pod-security-context/","text":"As Subkube is a shared platform, containers are only allowed to run in so-called 'restricted' PodSecurityPolicies. This guide walks through the steps of creating a deployment, seeing it fail, updating it to use a SecurityContext, and seeing it run successfully. Prerequisites A Namespace in a Subkube Project Kubectl Steps Setting up a basic Deployment For this example, we will use the echoserver image, which sets up a simple HTTP server which replies everything sent to it. deployment-before.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion : apps/v1 kind : Deployment metadata : creationTimestamp : null labels : app : hello-subkube name : hello-subkube spec : replicas : 1 selector : matchLabels : app : hello-subkube strategy : {} template : metadata : creationTimestamp : null labels : app : hello-subkube spec : containers : - image : ealen/echo-server:latest name : echoserver resources : {} status : {} Let's apply the supplied, unmodified deployment, to a namespace, called demo in our example: 1 kubectl apply -n demo -f deployment-before.yaml Initial testing Because we don't want to set up any Services or an Ingress, we'll use a PortForward to connect to the Deployment: 1 2 > kubectl port-forward -n demo deployment/hello-subkube 8080 :8080 error: unable to forward port because pod is not running. Current status = Pending \ud83d\ude15 Figuring out why our Pod fails Let's check the logs and see what is going wrong 1 2 > kubectl logs deployment/hello-subkube -n demo Error from server ( BadRequest ) : container \"echoserver\" in pod \"hello-subkube-758bc95559-bgg29\" is waiting to start: CreateContainerConfigError We are shown a CreateContainerConfigError while trying to fetch the Pod's logs. Let's look in to that: 1 2 3 4 5 6 > kubectl describe pods -n demo ... Normal Pulled 47s ( x8 over 2m18s ) kubelet, minikube-local-local Successfully pulled image \"ealen/echo-server:latest\" Warning Failed 47s ( x8 over 2m18s ) kubelet, minikube-local-local Error: container has runAsNonRoot and image will run as root Normal Pulling 33s ( x9 over 2m27s ) kubelet, minikube-local-local Pulling image \"ealen/echo-server:latest\" ... Because we haven't configured our Deployment to run with an appopriate SecurityContext, the Kubernets control plane isn't allowing Pods in our Deployment to run. Setting a SecurityContext We can easily fix this by setting a SecurityContext . For many containers this is as easy as setting the runAsUser property to the value 65534 , which translates to the nobody user account. By default, the echo-server image runs on port 80 , but because our container won't be running as root , the container will be unable to allocate the port, and the Pod will fail. Instead, we want to run the echo-server on a different port, say 8080. Let's update our Deployment manifest accordingly: deployment-after.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion : apps/v1 kind : Deployment metadata : creationTimestamp : null labels : app : hello-subkube name : hello-subkube spec : replicas : 1 selector : matchLabels : app : hello-subkube strategy : {} template : metadata : creationTimestamp : null labels : app : hello-subkube spec : containers : - image : ealen/echo-server:latest name : echoserver env : - name : PORT value : \"8080\" securityContext : runAsUser : 65534 status : {} Now we should apply the modified deployment: 1 kubectl apply -n demo -f deployment-after.yaml Testing our revised Deployment Let's again try to start our PortForward: 1 kubectl port-forward -n demo deployment/hello-subkube 8080 :8080 That should've worked. Now we can use curl to test our Deployment: 1 2 > curl 'localhost:8080/?echo_body=All_Good' All_Good More Resources & Further Reading K8s Concepts: Pod Security Policies K8s Concepts: Pod Security Standards Better Programming on Medium: Secure Your Kubernetes Cluster With Pod Security Policies","title":"Configure a Pod to run with a limited SecurityContext"},{"location":"guides/pod-security-context/#prerequisites","text":"A Namespace in a Subkube Project Kubectl","title":"Prerequisites"},{"location":"guides/pod-security-context/#steps","text":"","title":"Steps"},{"location":"guides/pod-security-context/#setting-up-a-basic-deployment","text":"For this example, we will use the echoserver image, which sets up a simple HTTP server which replies everything sent to it. deployment-before.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion : apps/v1 kind : Deployment metadata : creationTimestamp : null labels : app : hello-subkube name : hello-subkube spec : replicas : 1 selector : matchLabels : app : hello-subkube strategy : {} template : metadata : creationTimestamp : null labels : app : hello-subkube spec : containers : - image : ealen/echo-server:latest name : echoserver resources : {} status : {} Let's apply the supplied, unmodified deployment, to a namespace, called demo in our example: 1 kubectl apply -n demo -f deployment-before.yaml","title":"Setting up a basic Deployment"},{"location":"guides/pod-security-context/#initial-testing","text":"Because we don't want to set up any Services or an Ingress, we'll use a PortForward to connect to the Deployment: 1 2 > kubectl port-forward -n demo deployment/hello-subkube 8080 :8080 error: unable to forward port because pod is not running. Current status = Pending \ud83d\ude15","title":"Initial testing"},{"location":"guides/pod-security-context/#figuring-out-why-our-pod-fails","text":"Let's check the logs and see what is going wrong 1 2 > kubectl logs deployment/hello-subkube -n demo Error from server ( BadRequest ) : container \"echoserver\" in pod \"hello-subkube-758bc95559-bgg29\" is waiting to start: CreateContainerConfigError We are shown a CreateContainerConfigError while trying to fetch the Pod's logs. Let's look in to that: 1 2 3 4 5 6 > kubectl describe pods -n demo ... Normal Pulled 47s ( x8 over 2m18s ) kubelet, minikube-local-local Successfully pulled image \"ealen/echo-server:latest\" Warning Failed 47s ( x8 over 2m18s ) kubelet, minikube-local-local Error: container has runAsNonRoot and image will run as root Normal Pulling 33s ( x9 over 2m27s ) kubelet, minikube-local-local Pulling image \"ealen/echo-server:latest\" ... Because we haven't configured our Deployment to run with an appopriate SecurityContext, the Kubernets control plane isn't allowing Pods in our Deployment to run.","title":"Figuring out why our Pod fails"},{"location":"guides/pod-security-context/#setting-a-securitycontext","text":"We can easily fix this by setting a SecurityContext . For many containers this is as easy as setting the runAsUser property to the value 65534 , which translates to the nobody user account. By default, the echo-server image runs on port 80 , but because our container won't be running as root , the container will be unable to allocate the port, and the Pod will fail. Instead, we want to run the echo-server on a different port, say 8080. Let's update our Deployment manifest accordingly: deployment-after.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion : apps/v1 kind : Deployment metadata : creationTimestamp : null labels : app : hello-subkube name : hello-subkube spec : replicas : 1 selector : matchLabels : app : hello-subkube strategy : {} template : metadata : creationTimestamp : null labels : app : hello-subkube spec : containers : - image : ealen/echo-server:latest name : echoserver env : - name : PORT value : \"8080\" securityContext : runAsUser : 65534 status : {} Now we should apply the modified deployment: 1 kubectl apply -n demo -f deployment-after.yaml","title":"Setting a SecurityContext"},{"location":"guides/pod-security-context/#testing-our-revised-deployment","text":"Let's again try to start our PortForward: 1 kubectl port-forward -n demo deployment/hello-subkube 8080 :8080 That should've worked. Now we can use curl to test our Deployment: 1 2 > curl 'localhost:8080/?echo_body=All_Good' All_Good","title":"Testing our revised Deployment"},{"location":"guides/pod-security-context/#more-resources-further-reading","text":"K8s Concepts: Pod Security Policies K8s Concepts: Pod Security Standards Better Programming on Medium: Secure Your Kubernetes Cluster With Pod Security Policies","title":"More Resources &amp; Further Reading"},{"location":"guides/soon/","text":"Note Coming soon!","title":"Soon"}]}